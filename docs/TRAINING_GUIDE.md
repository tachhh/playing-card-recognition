# ‡∏Ñ‡∏π‡πà‡∏°‡∏∑‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Å‡∏≤‡∏£ Train Model ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å
## ‡∏à‡∏≤‡∏Å‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏™‡∏π‡πà AI ‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏°‡πâ‡πÄ‡∏•‡πà‡∏ô‡πÑ‡∏î‡πâ 93.58%

---

## ‡∏™‡∏≤‡∏£‡∏ö‡∏±‡∏ç

1. [‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô](#‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)
2. [Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£](#machine-learning-‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£)
3. [Neural Network: ‡∏™‡∏°‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ](#neural-network-‡∏™‡∏°‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏ó‡πÄ‡∏£‡∏¢‡∏ô‡∏£‡πÑ‡∏î)
4. [Linear Algebra ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Train Model](#linear-algebra-‡πÉ‡∏ô‡∏Å‡∏≤‡∏£-train-model)
5. [‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ Training ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î](#‡∏Ç‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£-training-‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏¢‡∏î)
6. [Backpropagation: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà AI ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ](#backpropagation-‡∏ß‡∏ò‡∏óa‡πÄ‡∏£‡∏¢‡∏ô‡∏£)
7. [Optimization ‡πÅ‡∏•‡∏∞ Learning Rate](#optimization-‡πÅ‡∏•‡∏∞-learning-rate)
8. [‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ](#‡∏õ‡∏ç‡∏´‡∏≤‡∏ó‡∏û‡∏ö‡∏ö‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏ß‡∏ò‡πÅ‡∏Å)

---

## ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô

### ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö‡∏™‡∏π‡πà AI ‡∏ó‡∏µ‡πà‡∏à‡∏î‡∏à‡∏≥‡πÑ‡∏°‡πâ‡πÄ‡∏•‡πà‡∏ô‡πÑ‡∏î‡πâ

```
[Dataset: 7,624 ‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πâ‡πÄ‡∏•‡πà‡∏ô]
‚Üì ‡πÅ‡∏ö‡πà‡∏á Train/Validation (80%/20%)
‚Üì ‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô 53 ‡∏Ñ‡∏•‡∏≤‡∏™
‚îÇ
‚îú‚îÄ Training Set (6,099 ‡∏†‡∏≤‡∏û)
‚îÇ  ‚Üì Data Augmentation
‚îÇ  ‚îú‚îÄ RandomHorizontalFlip ‚Üí ‡∏û‡∏•‡∏¥‡∏Å‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤
‚îÇ  ‚îú‚îÄ RandomRotation(¬±10¬∞) ‚Üí ‡∏´‡∏°‡∏∏‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
‚îÇ  ‚îî‚îÄ ColorJitter ‚Üí ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á
‚îÇ  ‚Üì Resize (224√ó224) + Normalize
‚îÇ  
‚îî‚îÄ Validation Set (1,525 ‡∏†‡∏≤‡∏û)
   ‚Üì Resize (224√ó224) + Normalize
   
[‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô Batch (32 ‡∏†‡∏≤‡∏û/‡∏Ñ‡∏£‡∏±‡πâ‡∏á)]
‚Üì DataLoader ‡∏™‡∏∏‡πà‡∏°‡∏†‡∏≤‡∏û‡∏ó‡∏∏‡∏Å epoch
‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  [Training Loop - 50 Epochs]        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                     ‚îÇ
‚îÇ  For each Batch (32 images):       ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ [Forward Pass] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  Input: [32, 3, 224, 224]    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì Conv1 (3‚Üí32 channels)   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì BatchNorm + ReLU         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì MaxPool (112√ó112)        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì Conv2 (32‚Üí64 channels)   ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì BatchNorm + ReLU         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì MaxPool (56√ó56)          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì Conv3 (64‚Üí128 channels)  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì BatchNorm + ReLU         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì MaxPool (28√ó28)          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì Conv4 (128‚Üí256 channels) ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì Bat          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì Flatten (50,176)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì FC1 (50,176‚Üí512) chNorm + ReLU         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì MaxPool (14√ó14)        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì ReLU + Dropout(0.5)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì FC2 (512‚Üí256)            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì ReLU + Dropout(0.5)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    ‚Üì FC3 (256‚Üí53)             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Output: [32, 53] logits      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ [Loss Calculation] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì Softmax (logits ‚Üí probs)  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì CrossEntropyLoss           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  L = -Œ£ y_true √ó log(y_pred)  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Loss = 2.35 (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á)      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ [Backward Pass] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì loss.backward()            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‚àÇL/‚àÇW ‡∏ó‡∏∏‡∏Å layer     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì ‡πÉ‡∏ä‡πâ Chain Rule             ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚àáW = [gradient tensors]      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ [Optimizer Step] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì Adam Optimizer              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  W_new = W - Œ± √ó ‚àáW           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì Update 26.7M parameters    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Repeat for all batches (191)     ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ [Validation Phase] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì torch.no_grad() (no train) ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì Forward Pass ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Accuracy              ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Valid Acc = 93.58%           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ [Learning Rate Scheduler] ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì ReduceLROnPlateau           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Üì ‡∏ñ‡πâ‡∏≤ Acc ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô 3 epochs  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  lr = lr √ó 0.5 (‡∏•‡∏î‡∏Ñ‡∏£‡∏∂‡πà‡∏á)      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ [Model Checkpoint] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  if valid_acc > best_acc:     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    Save model weights          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ    best_acc = valid_acc        ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

[Best Model Saved]
‚Üì models/card_classifier_cnn.pth
‚Üì Validation Accuracy: 93.58%
‚Üì 26,738,485 parameters
‚îÇ
‚îú‚îÄ class_to_idx.json (mapping)
‚îú‚îÄ training_history.json (metrics)
‚îî‚îÄ Ready for Inference!

[Inference (‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á)]
‚Üì ‡πÇ‡∏´‡∏•‡∏î trained model
‚Üì ‡πÉ‡∏™‡πà‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏°‡πà
‚îÇ
[‡∏Å‡∏•‡πâ‡∏≠‡∏á]
‚Üì ‡∏≠‡πà‡∏≤‡∏ô‡∏†‡∏≤‡∏û 30 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á/‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ
[OpenCV]
‚Üì ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û
‚îú‚îÄ Full Frame Mode ‚Üí ‡∏ï‡∏±‡∏î‡∏Å‡∏£‡∏≠‡∏ö‡∏ï‡∏£‡∏á‡∏Å‡∏•‡∏≤‡∏á
‚îî‚îÄ Auto Detect Mode ‚Üí ‡∏´‡∏≤ contour + ‡∏Ñ‡∏±‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
‚Üì ‡πÑ‡∏î‡πâ‡∏†‡∏≤‡∏û‡πÑ‡∏°‡πâ‡πÄ‡∏•‡πà‡∏ô
[Transform]
‚Üì Resize (224√ó224) + ToTensor + Normalize
[AI Model (CNN)]
‚Üì ‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ Neural Network
‚Üì ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 53 ‡∏Ñ‡∏•‡∏≤‡∏™
[Softmax]
‚Üì ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô %
[‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå]
‚îú‚îÄ Predicted Class: "ace_of_spades"
‚îú‚îÄ Confidence Score: 95.3%
‚îî‚îÄ Processing Time: ~33ms
‚Üì ‡∏ß‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ö‡∏ô‡∏†‡∏≤‡∏û
[‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠]
```

### ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Training vs Inference

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    TRAINING PHASE                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Purpose:     ‡∏™‡∏≠‡∏ô AI ‡πÉ‡∏´‡πâ‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å‡πÑ‡∏°‡πâ‡πÄ‡∏•‡πà‡∏ô‡∏ó‡∏±‡πâ‡∏á 53 ‡∏Ñ‡∏•‡∏≤‡∏™                ‚îÇ
‚îÇ Duration:    2-3 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á (50 epochs)                            ‚îÇ
‚îÇ Data:        7,624 ‡∏†‡∏≤‡∏û (‡πÅ‡∏ö‡πà‡∏á train/valid)                       ‚îÇ
‚îÇ Operations:  Forward + Backward + Weight Update                 ‚îÇ
‚îÇ Memory:      ~4 GB GPU RAM                                       ‚îÇ
‚îÇ Speed:       ~2 seconds/batch (32 images)                       ‚îÇ
‚îÇ Output:      Model weights (.pth file)                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   INFERENCE PHASE                               ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Purpose:     ‡πÉ‡∏ä‡πâ AI ‡∏ó‡∏µ‡πà train ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡πÉ‡∏´‡∏°‡πà                  ‚îÇ
‚îÇ Duration:    ~33 ms ‡∏ï‡πà‡∏≠‡∏†‡∏≤‡∏û (realtime)                          ‚îÇ
‚îÇ Data:        1 ‡∏†‡∏≤‡∏û‡∏ï‡πà‡∏≠‡∏Ñ‡∏£‡∏±‡πâ‡∏á                                      ‚îÇ
‚îÇ Operations:  Forward Pass ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô                              ‚îÇ
‚îÇ Memory:      ~500 MB RAM                                         ‚îÇ
‚îÇ Speed:       30 FPS (realtime video)                            ‚îÇ
‚îÇ Output:      Prediction + Confidence Score                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Timeline ‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

```
Epoch    Train Loss    Train Acc    Valid Loss    Valid Acc    LR
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
  1        3.850         12.5%        3.650         15.2%     0.0001
  5        2.120         45.8%        2.350         48.3%     0.0001
 10        1.250         65.3%        1.580         68.7%     0.0001
 15        0.820         78.9%        1.120         75.4%     0.0001
 20        0.520         85.6%        0.850         82.1%     0.0001
 25        0.350         91.2%        0.620         88.5%     0.00005 ‚Üê LR ‡∏•‡∏î
 30        0.180         95.4%        0.480         91.3%     0.00005
 35        0.120         97.1%        0.420         92.8%     0.000025 ‚Üê LR ‡∏•‡∏î
 40        0.085         98.2%        0.395         93.2%     0.000025
 45        0.065         98.8%        0.380         93.5%     0.000025
 50        0.052         99.1%        0.375         93.58%    0.000025 ‚úì
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:
  ‚Ä¢ Loss ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ (model ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
  ‚Ä¢ Accuracy ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ
  ‚Ä¢ Learning Rate ‡∏•‡∏î‡∏•‡∏á‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô (fine-tuning)
  ‚Ä¢ Train Acc > Valid Acc (overfitting ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏ï‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)
```

### Computational Complexity (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PER IMAGE (FORWARD PASS)                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Layer          Operations           FLOPs        Time        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Conv1 (3‚Üí32)   224¬≤√ó3√ó3¬≤√ó32        116M         2.5ms       ‚îÇ
‚îÇ Conv2 (32‚Üí64)  112¬≤√ó32√ó3¬≤√ó64       921M         8.1ms       ‚îÇ
‚îÇ Conv3 (64‚Üí128) 56¬≤√ó64√ó3¬≤√ó128       1,843M       12.3ms      ‚îÇ
‚îÇ Conv4 (128‚Üí256) 28¬≤√ó128√ó3¬≤√ó256     2,306M       14.8ms      ‚îÇ
‚îÇ FC1 (50K‚Üí512)  50,176√ó512          25.7M        1.2ms       ‚îÇ
‚îÇ FC2 (512‚Üí256)  512√ó256             131K         0.1ms       ‚îÇ
‚îÇ FC3 (256‚Üí53)   256√ó53              13.6K        0.05ms      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Total FLOPs:                       ~5.2 GFLOPS              ‚îÇ
‚îÇ Total Time (GPU):                  ~33 ms                    ‚îÇ
‚îÇ Total Time (CPU):                  ~280 ms                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FULL TRAINING (50 EPOCHS)                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Forward Pass:   6,099 images √ó 50 epochs = 304,950 passes   ‚îÇ
‚îÇ Backward Pass:  3√ó FLOPs of forward = 15.6 GFLOPS √ó 304K    ‚îÇ
‚îÇ Total FLOPs:    ~4.76 PFLOPS (Peta-FLOPS)                   ‚îÇ
‚îÇ GPU Time:       ~2 hours (RTX 3060)                          ‚îÇ
‚îÇ CPU Time:       ~24 hours (would be very slow!)             ‚îÇ
‚îÇ Power Usage:    ~0.4 kWh (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡πà‡∏≤‡πÑ‡∏ü 2 ‡∏ö‡∏≤‡∏ó)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Memory Usage (‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   TRAINING MEMORY                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Model Weights:         26.7M params √ó 4 bytes = 107 MB     ‚îÇ
‚îÇ Optimizer State:       √ó2 (momentum + variance) = 214 MB   ‚îÇ
‚îÇ Gradients:             26.7M √ó 4 bytes = 107 MB            ‚îÇ
‚îÇ Activations (batch):   ~1 GB (saved for backprop)          ‚îÇ
‚îÇ Input Batch:           32√ó3√ó224√ó224√ó4 = 192 MB             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Total GPU Memory:      ~1.6 GB                              ‚îÇ
‚îÇ Peak Usage:            ~2.5 GB (during backprop)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  INFERENCE MEMORY                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Model Weights:         107 MB                               ‚îÇ
‚îÇ Input Image:           3√ó224√ó224√ó4 = 600 KB                ‚îÇ
‚îÇ Activations:           ~50 MB (temporary)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Total RAM:             ~250 MB (CPU)                        ‚îÇ
‚îÇ Total GPU:             ~500 MB (GPU)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Machine Learning ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£

### ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢

**Machine Learning (ML)** = ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå **‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ** ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡∏ö‡∏≠‡∏Å‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á

**‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:**

```
‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ:
  if card == "ace of spades":
      return "ace of spades"
  elif card == "king of hearts":
      return "king of hearts"
  ... (‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô 53 ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç!) ‚ùå

Machine Learning:
  ‡πÉ‡∏´‡πâ AI ‡∏î‡∏π‡∏†‡∏≤‡∏û 7,624 ‡∏†‡∏≤‡∏û
  AI ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏≠‡∏á‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÉ‡∏ö‡∏´‡∏ô‡πâ‡∏≤‡∏ï‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£
  ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÑ‡∏î‡πâ 93.58% ‚úì ‚úÖ
```

### ‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡∏≠‡∏á Machine Learning

**1. Supervised Learning (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡∏°‡∏µ‡∏Ñ‡∏£‡∏π)**
```
‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö:
  ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà 1 + ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: "ace of spades"
  ‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà 2 + ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: "king of hearts"
  ...
  AI ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ
```

**2. Unsupervised Learning (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏£‡∏π)**
```
‡πÉ‡∏´‡πâ AI ‡∏´‡∏≤‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏≠‡∏á:
  ‡∏à‡∏±‡∏î‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÑ‡∏°‡πâ‡∏ó‡∏µ‡πà‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ô
  ‡πÑ‡∏°‡πà‡∏ö‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
```

**3. Reinforcement Learning (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏•‡∏≠‡∏á‡∏ú‡∏¥‡∏î‡∏•‡∏≠‡∏á‡∏ñ‡∏π‡∏Å)**
```
AI ‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏Å‡∏°:
  ‡∏ä‡∏ô‡∏∞ ‚Üí ‡πÑ‡∏î‡πâ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (+reward)
  ‡πÅ‡∏û‡πâ ‚Üí ‡πÄ‡∏™‡∏µ‡∏¢‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô (-reward)
  ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
```

**‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ:** Supervised Learning

---

## Neural Network: ‡∏™‡∏°‡∏≠‡∏á‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ

### Neuron (‡πÄ‡∏ã‡∏•‡∏•‡πå‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó)

**Neuron ‡πÉ‡∏ô‡∏™‡∏°‡∏≠‡∏á‡∏Ñ‡∏ô:**
```
[Input 1] ‚îÄ‚îÄ‚Üí
[Input 2] ‚îÄ‚îÄ‚Üí  [Neuron] ‚îÄ‚îÄ‚Üí Output
[Input 3] ‚îÄ‚îÄ‚Üí
```

**Artificial Neuron (‡πÄ‡∏ó‡∏µ‡∏¢‡∏°):**
```
[x‚ÇÅ] ‚îÄ‚îÄw‚ÇÅ‚Üí
[x‚ÇÇ] ‚îÄ‚îÄw‚ÇÇ‚Üí  Œ£ ‚Üí Activation ‚Üí Output
[x‚ÇÉ] ‚îÄ‚îÄw‚ÇÉ‚Üí

‡∏™‡∏°‡∏Å‡∏≤‡∏£:
  output = activation(w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + w‚ÇÉx‚ÇÉ + b)
  
‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
  x = input
  w = weight (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å, ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà AI ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ)
  b = bias
```

### Linear Algebra ‡πÉ‡∏ô Neuron

**‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Matrix:**
```
y = œÉ(Wx + b)

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
  x ‚àà ‚Ñù‚Åø     = input vector
  W ‚àà ‚Ñù·µêÀ£‚Åø   = weight matrix
  b ‚àà ‚Ñù·µê     = bias vector
  œÉ = activation function
  y ‚àà ‚Ñù·µê     = output vector
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:**
```python
# Input
x = [1, 2, 3]

# Weights
W = [[0.5, 0.3, 0.2],
     [0.1, 0.4, 0.6]]

# Bias
b = [0.1, 0.2]

# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Wx
Wx = W @ x = [0.5√ó1 + 0.3√ó2 + 0.2√ó3, 
              0.1√ó1 + 0.4√ó2 + 0.6√ó3]
   = [1.7, 2.7]

# ‡πÄ‡∏û‡∏¥‡πà‡∏° bias
Wx + b = [1.7 + 0.1, 2.7 + 0.2]
       = [1.8, 2.9]

# Activation (ReLU)
y = max(0, [1.8, 2.9]) = [1.8, 2.9]
```

### Neural Network = ‡∏´‡∏•‡∏≤‡∏¢ Neurons ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Å‡∏±‡∏ô

**Architecture ‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå:**

```
[Input: 224√ó224√ó3]
        ‚Üì
[Conv Layer 1: 3‚Üí32 channels]
  ‚Üì 32 Feature Maps
[Conv Layer 2: 32‚Üí64 channels]
  ‚Üì 64 Feature Maps
[Conv Layer 3: 64‚Üí128 channels]
  ‚Üì 128 Feature Maps
[Conv Layer 4: 128‚Üí256 channels]
  ‚Üì 256 Feature Maps
[Flatten: 256√ó14√ó14 ‚Üí 50,176]
        ‚Üì
[FC Layer 1: 50,176 ‚Üí 512]
  ‚Üì 512 neurons
[FC Layer 2: 512 ‚Üí 256]
  ‚Üì 256 neurons
[FC Layer 3: 256 ‚Üí 53]
  ‚Üì 53 outputs (1 per class)
[Softmax]
  ‚Üì 53 probabilities
[Predicted Class]
```

---

## Linear Algebra ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Train Model

### 1. Matrix Multiplication ‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞ Layer

**Fully Connected Layer:**
```
Input:  x ‚àà ‚Ñù‚Åµ‚Å∞¬π‚Å∑‚Å∂
Weight: W ‚àà ‚Ñù‚Åµ¬π¬≤À£‚Åµ‚Å∞¬π‚Å∑‚Å∂
Bias:   b ‚àà ‚Ñù‚Åµ¬π¬≤
Output: y ‚àà ‚Ñù‚Åµ¬π¬≤

‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:
  y = Wx + b
  
‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏π‡∏ì:
  512 √ó 50,176 = 25,690,112 operations!
```

**Convolutional Layer:**
```
Input:  I ‚àà ‚Ñù·¥¥À£·µÇÀ£·∂ú‚Å±‚Åø
Kernel: K ‚àà ‚Ñù·µèÀ£·µèÀ£·∂ú‚Å±‚ÅøÀ£·∂ú·µí·µò·µó
Output: O ‚àà ‚Ñù·¥¥'À£·µÇ'À£·∂ú·µí·µò·µó

‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:
  O = I ‚àó K (convolution)
  
‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ output pixel:
  O(i,j,c) = Œ£Œ£Œ£ I(i+m, j+n, d) √ó K(m,n,d,c)
```

### 2. Forward Pass (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏õ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)

**‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡πà‡∏≤‡∏ô Network:**

```
Layer 1:  a‚ÇÅ = œÉ(W‚ÇÅx + b‚ÇÅ)
Layer 2:  a‚ÇÇ = œÉ(W‚ÇÇa‚ÇÅ + b‚ÇÇ)
Layer 3:  a‚ÇÉ = œÉ(W‚ÇÉa‚ÇÇ + b‚ÇÉ)
...
Output:   y = softmax(W‚Çôa‚Çô‚Çã‚ÇÅ + b‚Çô)
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç:**
```
Input Image (224√ó224√ó3):
  [[[255, 128, 64], ...], ...]
         ‚Üì Normalize
  [[[0.98, 0.45, 0.21], ...], ...]
         ‚Üì Conv + ReLU
  [[0.0, 0.5, 1.2, ...], ...] (32 channels)
         ‚Üì Conv + ReLU
  [[0.0, 0.8, 0.3, ...], ...] (64 channels)
         ‚Üì ... (more layers)
  [0.1, 0.05, ..., 0.85, ...] (53 outputs)
         ‚Üì Softmax
  [0.001, 0.0005, ..., 0.95, ...] (probabilities)
         ‚Üì
  Predicted: class 42 (85% confidence)
  True label: class 42
  ‚Üí Correct! ‚úì
```

### 3. Loss Function (‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)

**Cross-Entropy Loss:**
```
L = -Œ£ y·µ¢ log(≈∑·µ¢)
    i=1 to n

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
  y·µ¢ = true label (one-hot encoded)
  ≈∑·µ¢ = predicted probability
  
‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
  True:      [0, 0, 1, 0, 0]  (class 2)
  Predicted: [0.1, 0.2, 0.6, 0.05, 0.05]
  
  Loss = -(0√ólog(0.1) + 0√ólog(0.2) + 1√ólog(0.6) + ...)
       = -log(0.6)
       = 0.51
       
  ‡∏ñ‡πâ‡∏≤ Predicted ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô: [0.01, 0.02, 0.95, 0.01, 0.01]
  Loss = -log(0.95) = 0.05  (‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤ = ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤!)
```

**‡∏ó‡∏≥‡πÑ‡∏°‡πÉ‡∏ä‡πâ Log?**
```
Probability    Log        Loss
0.99          -0.01       0.01  (‡∏î‡∏µ‡∏°‡∏≤‡∏Å)
0.9           -0.10       0.10
0.5           -0.69       0.69
0.1           -2.30       2.30
0.01          -4.61       4.61  (‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å)

Log ‡∏ó‡∏≥‡πÉ‡∏´‡πâ penalty ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏¥‡∏î‡πÄ‡∏¢‡∏≠‡∏∞
```

---

## ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£ Training ‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î

### Phase 1: Data Loading (‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•)

**1.1 Dataset Structure:**
```
data/train/
  ‚îú‚îÄ‚îÄ ace_of_spades/
  ‚îÇ   ‚îú‚îÄ‚îÄ img001.jpg
  ‚îÇ   ‚îú‚îÄ‚îÄ img002.jpg
  ‚îÇ   ‚îî‚îÄ‚îÄ ...
  ‚îú‚îÄ‚îÄ king_of_hearts/
  ‚îÇ   ‚îú‚îÄ‚îÄ img001.jpg
  ‚îÇ   ‚îî‚îÄ‚îÄ ...
  ‚îî‚îÄ‚îÄ ... (53 classes)
  
Total: 7,624 images
```

**1.2 Data Loading Process:**
```python
# 1. ‡∏™‡πÅ‡∏Å‡∏ô‡∏´‡∏≤‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
samples = []
for class_name in classes:
    for img_file in list_files(class_name):
        samples.append((img_file, class_name))

# 2. ‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
random.shuffle(samples)

# 3. ‡πÅ‡∏ö‡πà‡∏á Batch
batch_size = 32
batch = samples[0:32]  # 32 images
```

**1.3 Image Preprocessing:**
```
[Original Image: 500√ó700√ó3]
        ‚Üì Resize
[Resized: 224√ó224√ó3]
        ‚Üì Normalize (Œº, œÉ)
[Normalized: 224√ó224√ó3]
  Values: -2 ‡∏ñ‡∏∂‡∏á 2
        ‚Üì ToTensor
[Tensor: [1, 3, 224, 224]]
  Shape: [Batch, Channels, Height, Width]
```

**Matrix Operations:**
```
# Resize (Bilinear Interpolation)
I_new(i,j) = Œ£ Œ£ I(m,n) √ó weight(i,j,m,n)

# Normalize
I_norm(c) = (I(c) - Œº(c)) / œÉ(c)

Œº = [0.485, 0.456, 0.406]  (mean)
œÉ = [0.229, 0.224, 0.225]  (std)
```

### Phase 2: Forward Pass (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÑ‡∏õ‡∏Ç‡πâ‡∏≤‡∏á‡∏´‡∏ô‡πâ‡∏≤)

**2.1 Convolution Layer:**
```
Input:  [32, 3, 224, 224]   (batch, channels, H, W)
Weight: [32, 3, 3, 3]        (out_ch, in_ch, kH, kW)
Bias:   [32]

‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:
  for each image in batch:
    for each output channel:
      for each position (i,j):
        output[i,j] = Œ£ Œ£ Œ£ input[m,n,c] √ó kernel[m,n,c]
                      m n c
        output[i,j] += bias
        
Output: [32, 32, 224, 224]
```

**2.2 Batch Normalization:**
```
‡∏™‡∏°‡∏Å‡∏≤‡∏£:
  BN(x) = Œ≥ √ó ((x - Œº_batch) / ‚àö(œÉ¬≤_batch + Œµ)) + Œ≤

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
  Œº_batch = mean ‡∏Ç‡∏≠‡∏á batch ‡∏ô‡∏µ‡πâ
  œÉ_batch = std ‡∏Ç‡∏≠‡∏á batch ‡∏ô‡∏µ‡πâ
  Œ≥, Œ≤ = learnable parameters
  Œµ = 10‚Åª‚Åµ (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢ 0)

‡∏ó‡∏≥‡πÑ‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏ó‡∏≥?
  1. ‡∏ó‡∏≥‡πÉ‡∏´‡πâ training ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô
  2. ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô gradient vanishing
  3. ‡∏•‡∏î overfitting
```

**2.3 Activation Function (ReLU):**
```
ReLU(x) = max(0, x) = {
  x   if x > 0
  0   if x ‚â§ 0
}

‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:
  Input:  [-2, -1, 0, 1, 2]
  Output: [0, 0, 0, 1, 2]

Matrix Operation:
  ReLU(X) = X ‚äô (X > 0)
  ‚äô = element-wise multiplication
```

**2.4 MaxPooling:**
```
Input:  [4√ó4]
[1 2 | 3 4]
[5 6 | 7 8]
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
[9 10| 11 12]
[13 14| 15 16]

Output: [2√ó2]
[6  8]    ‚Üê ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÉ‡∏ô 2√ó2
[14 16]

‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå:
  1. ‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û ‚Üí ‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
  2. Translation invariance
  3. ‡∏•‡∏î overfitting
```

### Phase 3: Loss Calculation (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î)

**3.1 Softmax:**
```
Input (logits):  [2.3, 1.5, 4.8, 0.2, ...]  (53 values)

Softmax:
  p_i = e^(z_i) / Œ£ e^(z_j)
        j=1 to 53

‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:
  e^2.3 = 9.97
  e^1.5 = 4.48
  e^4.8 = 121.51
  e^0.2 = 1.22
  ...
  
  sum = 9.97 + 4.48 + 121.51 + 1.22 + ... = 150.0
  
  p‚ÇÄ = 9.97 / 150.0 = 0.066   (6.6%)
  p‚ÇÅ = 4.48 / 150.0 = 0.030   (3.0%)
  p‚ÇÇ = 121.51 / 150.0 = 0.810 (81.0%) ‚Üê highest
  p‚ÇÉ = 1.22 / 150.0 = 0.008   (0.8%)
  ...
  
Output: [0.066, 0.030, 0.810, 0.008, ...]
        Œ£ = 1.0 (100%)
```

**3.2 Cross-Entropy Loss:**
```
True label: class 2 (ace of spades)
One-hot:    [0, 0, 1, 0, 0, ..., 0]  (53 values)
Predicted:  [0.066, 0.030, 0.810, 0.008, ...]

Loss = -Œ£ y_i √ó log(≈∑_i)
     = -(0√ólog(0.066) + 0√ólog(0.030) + 1√ólog(0.810) + ...)
     = -log(0.810)
     = 0.211

‡∏ñ‡πâ‡∏≤ AI ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ñ‡∏π‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô:
  Predicted: [0.01, 0.01, 0.95, 0.01, ...]
  Loss = -log(0.95) = 0.051  (‡∏•‡∏î‡∏•‡∏á!)
```

---

## Backpropagation: ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà AI ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ

### Gradient Descent (‡∏•‡∏á‡πÄ‡∏Ç‡∏≤‡∏´‡∏≤ Minimum)

**‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö:**
```
‡∏•‡∏≠‡∏á‡∏ô‡∏∂‡∏Å‡∏†‡∏≤‡∏û‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏π‡πà‡∏ö‡∏ô‡∏†‡∏π‡πÄ‡∏Ç‡∏≤ (Loss ‡∏™‡∏π‡∏á)
‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏á‡πÑ‡∏õ‡∏´‡∏≤‡∏´‡∏∏‡∏ö‡πÄ‡∏Ç‡∏≤ (Loss ‡∏ï‡πà‡∏≥)
‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏´‡πá‡∏ô‡∏ó‡∏≤‡∏á (‡∏ï‡∏≤‡∏ö‡∏≠‡∏î)

‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£:
  1. ‡∏¢‡∏∑‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∏‡∏î‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
  2. ‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ‡∏£‡∏≠‡∏ö‡πÜ ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢
  3. ‡∏´‡∏≤‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡πà‡∏≥‡∏•‡∏á (gradient)
  4. ‡πÄ‡∏î‡∏¥‡∏ô‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡∏ô‡∏±‡πâ‡∏ô
  5. ‡∏ó‡∏≥‡∏ã‡πâ‡∏≥‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏à‡∏∏‡∏î‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î
```

**‡∏™‡∏°‡∏Å‡∏≤‡∏£:**
```
W_new = W_old - Œ± √ó ‚àáL

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
  W = weights (‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å)
  Œ± = learning rate (‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡πâ‡∏≤‡∏ß)
  ‚àáL = gradient ‡∏Ç‡∏≠‡∏á loss
```

### Chain Rule (‡∏Å‡∏é‡∏•‡∏π‡∏Å‡πÇ‡∏ã‡πà)

**‡∏õ‡∏±‡∏ç‡∏´‡∏≤:** Neural Network ‡∏°‡∏µ‡∏´‡∏•‡∏≤‡∏¢ layers ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏Å‡∏±‡∏ô

**Chain Rule ‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏Å‡πâ:**
```
‚àÇL/‚àÇW‚ÇÅ = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇa‚ÇÉ √ó ‚àÇa‚ÇÉ/‚àÇa‚ÇÇ √ó ‚àÇa‚ÇÇ/‚àÇa‚ÇÅ √ó ‚àÇa‚ÇÅ/‚àÇW‚ÇÅ

‡∏Ñ‡∏∑‡∏≠:
  gradient ‡∏Ç‡∏≠‡∏á Loss ‡∏ï‡πà‡∏≠ W‚ÇÅ
  = ‡∏Ñ‡∏π‡∏ì gradient ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà output ‡∏¢‡πâ‡∏≠‡∏ô‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤
```

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì:**

```
Layer 3:  y = W‚ÇÉa‚ÇÇ + b‚ÇÉ
Layer 2:  a‚ÇÇ = œÉ(W‚ÇÇa‚ÇÅ + b‚ÇÇ)
Layer 1:  a‚ÇÅ = œÉ(W‚ÇÅx + b‚ÇÅ)
Loss:     L = CrossEntropy(y, target)

Backprop:
  1. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‚àÇL/‚àÇy (gradient ‡∏Ç‡∏≠‡∏á loss)
     ‚àÇL/‚àÇy = y - target  (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CrossEntropy + Softmax)
     
  2. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‚àÇL/‚àÇW‚ÇÉ
     ‚àÇL/‚àÇW‚ÇÉ = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇW‚ÇÉ
            = (y - target) √ó a‚ÇÇ·µÄ
     
  3. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‚àÇL/‚àÇa‚ÇÇ
     ‚àÇL/‚àÇa‚ÇÇ = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇa‚ÇÇ
            = W‚ÇÉ·µÄ √ó (y - target)
     
  4. ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‚àÇL/‚àÇW‚ÇÇ
     ‚àÇL/‚àÇW‚ÇÇ = ‚àÇL/‚àÇa‚ÇÇ √ó ‚àÇa‚ÇÇ/‚àÇW‚ÇÇ
            = ‚àÇL/‚àÇa‚ÇÇ √ó œÉ'(W‚ÇÇa‚ÇÅ + b‚ÇÇ) √ó a‚ÇÅ·µÄ
     
  ... (‡∏¢‡πâ‡∏≠‡∏ô‡πÑ‡∏õ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ)
```

### Update Weights (‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å)

**‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç:**

```python
# Before training
W = [[0.5, 0.3],
     [0.2, 0.4]]

# Forward pass ‚Üí Loss = 2.5
# Backward pass ‚Üí Gradient
‚àáW = [[0.1, -0.2],
      [0.3, 0.15]]

# Update (learning_rate = 0.01)
W_new = W - 0.01 √ó ‚àáW
      = [[0.5, 0.3],     [[0.001, -0.002],
         [0.2, 0.4]]  -   [0.003,  0.0015]]
      
      = [[0.499, 0.302],
         [0.197, 0.3985]]

# Forward pass again ‚Üí Loss = 2.3 (‡∏•‡∏î‡∏•‡∏á!)
```

---

## Optimization ‡πÅ‡∏•‡∏∞ Learning Rate

### Learning Rate (‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡πâ‡∏≤‡∏ß)

**‡∏°‡∏µ‡∏ú‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?**

```
Learning Rate ‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Œ± = 0.00001):
  Step 1: Loss = 2.5 ‚Üí 2.499
  Step 2: Loss = 2.499 ‚Üí 2.498
  Step 100: Loss = 2.4
  ‚Üí ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å! ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô

Learning Rate ‡∏û‡∏≠‡∏î‡∏µ (Œ± = 0.0001):
  Step 1: Loss = 2.5 ‚Üí 2.3
  Step 2: Loss = 2.3 ‚Üí 2.0
  Step 10: Loss = 1.2
  ‚Üí ‡πÄ‡∏£‡πá‡∏ß‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£ ‚úì

Learning Rate ‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Œ± = 0.1):
  Step 1: Loss = 2.5 ‚Üí 5.0
  Step 2: Loss = 5.0 ‚Üí 12.0
  Step 3: Loss = 12.0 ‚Üí NaN
  ‚Üí ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡∏Ç‡πâ‡∏≤‡∏° minimum!
```

**‡∏Å‡∏£‡∏≤‡∏ü:**
```
Loss
  ^
  ‚îÇ     Too small
  ‚îÇ     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚îÇ    /
  ‚îÇ   /
  ‚îÇ  /
  ‚îÇ /________________
  ‚îÇ        
  ‚îÇ     Just right
  ‚îÇ     ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚îÇ    ‚ï±
  ‚îÇ   ‚ï±
  ‚îÇ  ‚ï±________
  ‚îÇ     
  ‚îÇ     Too large
  ‚îÇ     ‚ï±‚ï≤
  ‚îÇ    ‚ï±  ‚ï≤  ‚ï±‚ï≤
  ‚îÇ   ‚ï±    ‚ï≤‚ï±  ‚ï≤
  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Steps
```

### Adam Optimizer

**‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Simple Gradient Descent ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?**

**1. Momentum (‡πÇ‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡∏±‡∏°):**
```
‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏•‡∏π‡∏Å‡∏ö‡∏≠‡∏•‡∏Å‡∏•‡∏¥‡πâ‡∏á‡∏•‡∏á‡πÄ‡∏Ç‡∏≤:
  - ‡πÄ‡∏£‡πá‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏¢‡πÜ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô
  - ‡πÑ‡∏°‡πà‡∏´‡∏¢‡∏∏‡∏î‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏à‡∏≠‡∏´‡∏•‡∏∏‡∏°‡πÄ‡∏•‡πá‡∏Å‡πÜ
  
m_t = Œ≤‚ÇÅ √ó m_{t-1} + (1 - Œ≤‚ÇÅ) √ó ‚àáW_t
W_t = W_{t-1} - Œ± √ó m_t
```

**2. Adaptive Learning Rate:**
```
‡πÅ‡∏ï‡πà‡∏•‡∏∞ parameter ‡∏°‡∏µ learning rate ‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô:
  - Parameter ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ö‡πà‡∏≠‡∏¢ ‚Üí ‡∏•‡∏î learning rate
  - Parameter ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ô‡πâ‡∏≠‡∏¢ ‚Üí ‡πÄ‡∏û‡∏¥‡πà‡∏° learning rate
  
v_t = Œ≤‚ÇÇ √ó v_{t-1} + (1 - Œ≤‚ÇÇ) √ó (‚àáW_t)¬≤
W_t = W_{t-1} - Œ± √ó m_t / (‚àöv_t + Œµ)
```

**‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡πá‡∏°‡∏Ç‡∏≠‡∏á Adam:**
```
m_t = Œ≤‚ÇÅ √ó m_{t-1} + (1 - Œ≤‚ÇÅ) √ó ‚àáW_t           (momentum)
v_t = Œ≤‚ÇÇ √ó v_{t-1} + (1 - Œ≤‚ÇÇ) √ó (‚àáW_t)¬≤       (adaptive)

mÃÇ_t = m_t / (1 - Œ≤‚ÇÅ·µó)                         (bias correction)
vÃÇ_t = v_t / (1 - Œ≤‚ÇÇ·µó)

W_t = W_{t-1} - Œ± √ó mÃÇ_t / (‚àövÃÇ_t + Œµ)

‡πÇ‡∏î‡∏¢‡∏ó‡∏µ‡πà:
  Œ≤‚ÇÅ = 0.9  (momentum decay)
  Œ≤‚ÇÇ = 0.999 (variance decay)
  Œµ = 10‚Åª‚Å∏
```

### Learning Rate Scheduler

**ReduceLROnPlateau:**
```
‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô: lr = 0.0001

Epoch 1-5:   Valid Acc = 50% ‚Üí 70% (‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)
             lr = 0.0001 (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)

Epoch 6-8:   Valid Acc = 70% ‚Üí 71% (‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏ä‡πâ‡∏≤)
             lr = 0.0001 (‡∏£‡∏≠‡∏î‡∏π)

Epoch 9-11:  Valid Acc = 71% ‚Üí 71% (‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô 3 epochs)
             lr = 0.00005 (‡∏•‡∏î‡∏Ñ‡∏£‡∏∂‡πà‡∏á!)

Epoch 12-15: Valid Acc = 71% ‚Üí 75% (‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á)
             lr = 0.00005 (‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô)
```

---

## ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ

### 1. Overfitting (‡∏à‡∏≥‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Training Set)

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:**
```
Training Accuracy:   99%  (‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å)
Validation Accuracy: 65%  (‡∏ï‡πà‡∏≥)

‚Üí Model ‡∏à‡∏≥‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö
```

**‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏:**
- Model ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- Data ‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
- Train ‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**

**1. Dropout:**
```
Training:
  [Input] ‚Üí [Layer 1] ‚Üí Dropout(0.5) ‚Üí [Layer 2]
  
  Dropout(0.5):
    ‡∏™‡∏∏‡πà‡∏°‡∏õ‡∏¥‡∏î 50% ‡∏Ç‡∏≠‡∏á neurons
    [1, 2, 3, 4, 5] ‚Üí [1, 0, 3, 0, 5]
    
  ‡∏ó‡∏≥‡πÉ‡∏´‡πâ Model ‡πÑ‡∏°‡πà‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤ neuron ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß

Testing:
  ‡πÑ‡∏°‡πà‡∏°‡∏µ Dropout (‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å neurons)
```

**2. Data Augmentation:**
```python
transforms.Compose([
    transforms.RandomHorizontalFlip(p=0.5),  # ‡∏û‡∏•‡∏¥‡∏Å‡∏ã‡πâ‡∏≤‡∏¢-‡∏Ç‡∏ß‡∏≤
    transforms.RandomRotation(10),           # ‡∏´‡∏°‡∏∏‡∏ô ¬±10¬∞
    transforms.ColorJitter(brightness=0.2),  # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡πà‡∏≤‡∏á
])

‡∏†‡∏≤‡∏û‡πÄ‡∏î‡∏¥‡∏°:      üÇ°
Flip:         üÇ° (‡∏û‡∏•‡∏¥‡∏Å)
Rotate:       üÇ° (‡πÄ‡∏≠‡∏µ‡∏¢‡∏á 5¬∞)
Brightness:   üÇ° (‡∏™‡∏ß‡πà‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô)

‚Üí 1 ‡∏†‡∏≤‡∏û ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏†‡∏≤‡∏û!
```

### 2. Underfitting (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ)

**‡∏≠‡∏≤‡∏Å‡∏≤‡∏£:**
```
Training Accuracy:   40%  (‡∏ï‡πà‡∏≥)
Validation Accuracy: 38%  (‡∏ï‡πà‡∏≥)

‚Üí Model ‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏°‡πà‡∏û‡∏≠
```

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
- ‡πÄ‡∏û‡∏¥‡πà‡∏° layers / neurons
- Train ‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô
- ‡∏•‡∏î regularization
- ‡πÄ‡∏û‡∏¥‡πà‡∏° learning rate

### 3. Gradient Vanishing/Exploding

**Vanishing (‡∏´‡∏≤‡∏¢‡πÑ‡∏õ):**
```
Layer 10: gradient = 0.5
Layer 9:  gradient = 0.5 √ó 0.5 = 0.25
Layer 8:  gradient = 0.25 √ó 0.5 = 0.125
...
Layer 1:  gradient = 0.0009  (‡πÄ‡∏Å‡∏∑‡∏≠‡∏ö 0!)

‚Üí Layer ‡πÅ‡∏£‡∏Å‡πÜ ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å
```

**Exploding (‡∏£‡∏∞‡πÄ‡∏ö‡∏¥‡∏î):**
```
Layer 10: gradient = 2.0
Layer 9:  gradient = 2.0 √ó 2.0 = 4.0
Layer 8:  gradient = 4.0 √ó 2.0 = 8.0
...
Layer 1:  gradient = 1024  (‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å!)

‚Üí Weights ‡∏Å‡∏£‡∏∞‡πÇ‡∏î‡∏î‡πÑ‡∏õ‡∏°‡∏≤, Loss = NaN
```

**‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ:**
- ‡πÉ‡∏ä‡πâ Batch Normalization
- ‡πÉ‡∏ä‡πâ ReLU ‡πÅ‡∏ó‡∏ô Sigmoid/Tanh
- ‡πÉ‡∏ä‡πâ Skip Connections (ResNet)
- Gradient Clipping

---

## ‡∏™‡∏£‡∏∏‡∏õ: ‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£ Training

```
[1. Data Loading]
   ‚Üì ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û 7,624 ‡∏†‡∏≤‡∏û
   ‚Üì ‡πÅ‡∏ö‡πà‡∏á Train/Valid
   ‚Üì Batch size = 32
   
[2. Initialize Model]
   ‚Üì Random Weights W ~ N(0, 0.01)
   ‚Üì 26M parameters
   
[3. Training Loop (50 epochs)]
   ‚îÇ
   ‚îú‚îÄ For each epoch:
   ‚îÇ  ‚îÇ
   ‚îÇ  ‚îú‚îÄ For each batch (32 images):
   ‚îÇ  ‚îÇ  ‚îÇ
   ‚îÇ  ‚îÇ  ‚îú‚îÄ [Forward Pass]
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì Conv Layer 1: I ‚àó K‚ÇÅ + b‚ÇÅ
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì BatchNorm + ReLU
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì MaxPool
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì Conv Layer 2, 3, 4...
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì Flatten
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì FC Layers: Y = WX + b
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì Softmax
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì Output: probabilities
   ‚îÇ  ‚îÇ  ‚îÇ
   ‚îÇ  ‚îÇ  ‚îú‚îÄ [Loss Calculation]
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì L = -Œ£ y log(≈∑)
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì Average over batch
   ‚îÇ  ‚îÇ  ‚îÇ
   ‚îÇ  ‚îÇ  ‚îú‚îÄ [Backward Pass]
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì ‚àÇL/‚àÇW (Chain Rule)
   ‚îÇ  ‚îÇ  ‚îÇ  ‚Üì Compute gradients
   ‚îÇ  ‚îÇ  ‚îÇ
   ‚îÇ  ‚îÇ  ‚îî‚îÄ [Update Weights]
   ‚îÇ  ‚îÇ     ‚Üì W = W - Œ± √ó ‚àáW (Adam)
   ‚îÇ  ‚îÇ     ‚Üì Update 26M parameters
   ‚îÇ  ‚îÇ
   ‚îÇ  ‚îî‚îÄ [Validation]
   ‚îÇ     ‚Üì Forward Pass (no training)
   ‚îÇ     ‚Üì Calculate Accuracy
   ‚îÇ     ‚Üì Save if best model
   ‚îÇ
   ‚îî‚îÄ Repeat 50 times
   
[4. Final Result]
   ‚Üì Best Model: 93.58% accuracy
   ‚Üì Save to models/card_classifier_cnn.pth
   ‚Üì Ready for inference!
```

### Key Metrics

```
üìä Training Statistics:
   ‚Ä¢ Total Epochs: 50
   ‚Ä¢ Batch Size: 32
   ‚Ä¢ Learning Rate: 0.0001
   ‚Ä¢ Optimizer: Adam
   ‚Ä¢ Total Training Time: ~2 hours (GPU)
   
   Epoch 1:  Train 12% ‚Üí Valid 15%
   Epoch 10: Train 65% ‚Üí Valid 68%
   Epoch 20: Train 85% ‚Üí Valid 82%
   Epoch 30: Train 95% ‚Üí Valid 91%
   Epoch 50: Train 98% ‚Üí Valid 93.58% ‚úì
   
üìà Model Capacity:
   ‚Ä¢ Total Parameters: 26,738,485
   ‚Ä¢ Memory Usage: ~100 MB
   ‚Ä¢ Inference Time: ~33 ms per image
   
üéØ Performance:
   ‚Ä¢ Validation Accuracy: 93.58%
   ‚Ä¢ Test Accuracy: ~93%
   ‚Ä¢ Real-world Performance: 90-95%
```

---

**‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÇ‡∏î‡∏¢:** Playing Card Recognition Project  
**‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î:** October 14, 2025  
**‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö:** ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ô

---

¬© 2025 - ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏à‡∏±‡∏¢
